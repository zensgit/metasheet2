name: Attendance Remote Storage Health (Prod)

# Mark drill runs explicitly so dashboards can ignore them.
run-name: Attendance Remote Storage Health (Prod)${{ github.event_name == 'workflow_dispatch' && inputs.drill_fail == 'true' && ' [DRILL]' || '' }}${{ github.event_name == 'workflow_dispatch' && inputs.skip_host_sync == 'true' && ' [DEBUG]' || '' }}

on:
  workflow_dispatch:
    inputs:
      drill_fail:
        description: 'Optional: intentionally fail after storage check for FAIL-path validation (true/false)'
        required: false
        default: 'false'
      issue_title:
        description: 'Optional: override issue title for workflow_dispatch drills (leave empty for production title)'
        required: false
        default: ''
      skip_host_sync:
        description: 'Optional: skip deploy-host git sync step (true/false). Use for debugging when host sync is broken.'
        required: false
        default: 'false'
      max_fs_used_pct:
        description: 'Fail when filesystem usage percent is >= this value (df used_pct)'
        required: false
        default: '90'
      max_upload_dir_gb:
        description: 'Fail when upload dir size is >= this value (GB)'
        required: false
        default: '10'
      max_oldest_file_days:
        description: 'Fail when oldest file age is >= this value (days)'
        required: false
        default: '14'
  schedule:
    # Daily at 02:12 UTC (after remote metrics at 02:10 UTC; before strict gates at 02:15 UTC).
    - cron: '12 2 * * *'

concurrency:
  group: attendance-remote-storage-prod
  cancel-in-progress: true

permissions:
  contents: read
  issues: write

jobs:
  storage:
    runs-on: ubuntu-latest
    timeout-minutes: 15
    outputs:
      storage_rc: ${{ steps.remote_storage.outputs.storage_rc }}
      storage_reason: ${{ steps.remote_storage.outputs.storage_reason }}
      df_used_pct: ${{ steps.remote_storage.outputs.df_used_pct }}
      upload_gb: ${{ steps.remote_storage.outputs.upload_gb }}
      file_count: ${{ steps.remote_storage.outputs.file_count }}
      oldest_file_days: ${{ steps.remote_storage.outputs.oldest_file_days }}
    steps:
      - name: Remote storage check (with host sync logs)
        id: remote_storage
        continue-on-error: true
        env:
          DEPLOY_HOST: ${{ secrets.DEPLOY_HOST }}
          DEPLOY_USER: ${{ secrets.DEPLOY_USER }}
          DEPLOY_SSH_KEY_B64: ${{ secrets.DEPLOY_SSH_KEY_B64 }}
          DEPLOY_PATH: ${{ secrets.DEPLOY_PATH }}
          DEPLOY_COMPOSE_FILE: ${{ secrets.DEPLOY_COMPOSE_FILE }}
          DRILL_FAIL: ${{ github.event.inputs.drill_fail || 'false' }}
          SKIP_HOST_SYNC: ${{ github.event.inputs.skip_host_sync || 'false' }}
          MAX_FS_USED_PCT: ${{ github.event.inputs.max_fs_used_pct || '90' }}
          MAX_UPLOAD_DIR_GB: ${{ github.event.inputs.max_upload_dir_gb || '10' }}
          MAX_OLDEST_FILE_DAYS: ${{ github.event.inputs.max_oldest_file_days || '14' }}
        run: |
          set -euo pipefail
          mkdir -p ~/.ssh
          printf '%s' "$DEPLOY_SSH_KEY_B64" | base64 -d > ~/.ssh/deploy_key
          chmod 600 ~/.ssh/deploy_key
          ssh_opts="-o StrictHostKeyChecking=no -o IdentitiesOnly=yes -i ~/.ssh/deploy_key"

          DEPLOY_PATH="${DEPLOY_PATH:-metasheet2}"
          DEPLOY_COMPOSE_FILE="${DEPLOY_COMPOSE_FILE:-docker-compose.app.yml}"
          DRILL_FAIL="${DRILL_FAIL:-false}"
          SKIP_HOST_SYNC="${SKIP_HOST_SYNC:-false}"

          MAX_FS_USED_PCT="${MAX_FS_USED_PCT:-90}"
          MAX_UPLOAD_DIR_GB="${MAX_UPLOAD_DIR_GB:-10}"
          MAX_OLDEST_FILE_DAYS="${MAX_OLDEST_FILE_DAYS:-14}"

          mkdir -p output/storage
          storage_log="output/storage/storage.log"

          remote_cmds=(
            "set -euo pipefail"
            "DEPLOY_PATH=\"${DEPLOY_PATH}\""
            "DEPLOY_COMPOSE_FILE=\"${DEPLOY_COMPOSE_FILE}\""
            "DRILL_FAIL=\"${DRILL_FAIL}\""
            "SKIP_HOST_SYNC=\"${SKIP_HOST_SYNC}\""
            "MAX_FS_USED_PCT=\"${MAX_FS_USED_PCT}\""
            "MAX_UPLOAD_DIR_GB=\"${MAX_UPLOAD_DIR_GB}\""
            "MAX_OLDEST_FILE_DAYS=\"${MAX_OLDEST_FILE_DAYS}\""
            "cd \"${DEPLOY_PATH}\""
            "echo \"=== HOST SYNC START ===\""
            "host_sync_rc=0"
            "host_sync_attempts=3"
            'if [[ "${SKIP_HOST_SYNC}" == "true" ]]; then'
              '  echo "[host-sync] skipped (skip_host_sync=true)"'
              '  host_sync_rc=0'
            "else"
              "  set +e"
              '  for attempt in $(seq 1 "$host_sync_attempts"); do'
              '    git rev-parse --is-inside-work-tree >/dev/null 2>&1 && git fetch origin main && git checkout main && git pull --ff-only origin main'
              '    host_sync_rc=$?'
              '    echo "[host-sync] attempt=$attempt rc=$host_sync_rc"'
              '    if [[ "$host_sync_rc" == "0" ]]; then break; fi'
              '    if [[ "$attempt" -lt "$host_sync_attempts" ]]; then sleep $((attempt * 2)); fi'
              '  done'
              "  set -e"
            "fi"
            'echo "[host-sync] rc=$host_sync_rc attempts=$host_sync_attempts"'
            "echo \"=== HOST SYNC END ===\""
            "echo \"=== ATTENDANCE STORAGE START ===\""
            "storage_rc=0"
            'if [[ "$host_sync_rc" != "0" ]]; then'
              '  echo "[attendance-storage][skip] host sync failed: rc=$host_sync_rc"'
              '  storage_rc="$host_sync_rc"'
            "else"
              "  set +e"
              "  COMPOSE_FILE=\"${DEPLOY_COMPOSE_FILE}\" ENV_FILE=\"docker/app.env\" MAX_FS_USED_PCT=\"${MAX_FS_USED_PCT}\" MAX_UPLOAD_DIR_GB=\"${MAX_UPLOAD_DIR_GB}\" MAX_OLDEST_FILE_DAYS=\"${MAX_OLDEST_FILE_DAYS}\" scripts/ops/attendance-check-storage.sh"
              '  storage_rc=$?'
              "  set -e"
            "fi"
            "echo \"=== ATTENDANCE STORAGE END ===\""
            'if [[ "$storage_rc" != "0" ]]; then exit "$storage_rc"; fi'
            "if [[ \"${DRILL_FAIL}\" == \"true\" ]]; then echo \"[storage][drill] intentional failure after storage check\"; exit 97; fi"
          )

          set +e
          printf '%s\n' "${remote_cmds[@]}" | ssh $ssh_opts "$DEPLOY_USER@$DEPLOY_HOST" bash -s 2>&1 | tee "$storage_log"
          rc=${PIPESTATUS[1]}
          set -e

          df_used_pct="$(
            (grep -F "[attendance-storage] df_used_pct=" "$storage_log" 2>/dev/null | tail -n 1 | sed -E 's/.*df_used_pct=([0-9]+).*/\1/') || true
          )"
          upload_gb="$(
            (grep -F "[attendance-storage] upload_bytes=" "$storage_log" 2>/dev/null | tail -n 1 | sed -E 's/.*upload_gb=([0-9]+).*/\1/') || true
          )"
          file_count="$(
            (grep -F "[attendance-storage] file_count=" "$storage_log" 2>/dev/null | tail -n 1 | sed -E 's/.*file_count=([0-9]+).*/\1/') || true
          )"
          oldest_file_days="$(
            (grep -F "[attendance-storage] file_count=" "$storage_log" 2>/dev/null | tail -n 1 | sed -E 's/.*oldest_file_days=([0-9]+).*/\1/') || true
          )"

          reason="REMOTE_FAILED"
          if [[ "$rc" == "0" ]]; then
            reason="PASS"
          elif [[ "$rc" == "97" ]]; then
            reason="DRILL_FAIL"
          elif [[ "$rc" == "255" ]]; then
            reason="SSH_FAILED"
          elif [[ -f "$storage_log" ]] && grep -Fq "[attendance-storage][skip] host sync failed" "$storage_log"; then
            reason="HOST_SYNC_FAILED"
          elif [[ -f "$storage_log" ]] && grep -Fq "[attendance-storage] ERROR:" "$storage_log"; then
            reasons=()
            if grep -Fq "[attendance-storage] ERROR: filesystem usage too high" "$storage_log"; then
              reasons+=("FS_USAGE_TOO_HIGH")
            fi
            if grep -Fq "[attendance-storage] ERROR: upload dir too large" "$storage_log"; then
              reasons+=("UPLOAD_DIR_TOO_LARGE")
            fi
            if grep -Fq "[attendance-storage] ERROR: oldest file too old" "$storage_log"; then
              reasons+=("OLDEST_FILE_TOO_OLD")
            fi
            if (( ${#reasons[@]} > 0 )); then
              reason="$(IFS=,; echo "${reasons[*]}")"
            else
              reason="CHECK_FAILED"
            fi
          fi

          echo "storage_rc=$rc" >> "$GITHUB_OUTPUT"
          echo "storage_reason=$reason" >> "$GITHUB_OUTPUT"
          [[ -n "$df_used_pct" ]] && echo "df_used_pct=$df_used_pct" >> "$GITHUB_OUTPUT"
          [[ -n "$upload_gb" ]] && echo "upload_gb=$upload_gb" >> "$GITHUB_OUTPUT"
          [[ -n "$file_count" ]] && echo "file_count=$file_count" >> "$GITHUB_OUTPUT"
          [[ -n "$oldest_file_days" ]] && echo "oldest_file_days=$oldest_file_days" >> "$GITHUB_OUTPUT"
          echo "storage_log=$storage_log" >> "$GITHUB_OUTPUT"
          exit 0

      - name: Write step summary (storage + runbooks)
        if: always()
        env:
          STORAGE_RC: ${{ steps.remote_storage.outputs.storage_rc }}
          STORAGE_REASON: ${{ steps.remote_storage.outputs.storage_reason }}
          DRILL_FAIL: ${{ github.event.inputs.drill_fail || 'false' }}
          SKIP_HOST_SYNC: ${{ github.event.inputs.skip_host_sync || 'false' }}
          DF_USED_PCT: ${{ steps.remote_storage.outputs.df_used_pct }}
          UPLOAD_GB: ${{ steps.remote_storage.outputs.upload_gb }}
          FILE_COUNT: ${{ steps.remote_storage.outputs.file_count }}
          OLDEST_FILE_DAYS: ${{ steps.remote_storage.outputs.oldest_file_days }}
          MAX_FS_USED_PCT: ${{ github.event.inputs.max_fs_used_pct || '90' }}
          MAX_UPLOAD_DIR_GB: ${{ github.event.inputs.max_upload_dir_gb || '10' }}
          MAX_OLDEST_FILE_DAYS: ${{ github.event.inputs.max_oldest_file_days || '14' }}
        run: |
          set -euo pipefail
          storage_log="output/storage/storage.log"
          repo_url="${GITHUB_SERVER_URL}/${GITHUB_REPOSITORY}"
          artifact_name="attendance-remote-storage-prod-${GITHUB_RUN_ID}-${GITHUB_RUN_ATTEMPT}"

          overall="UNKNOWN"
          if [[ -n "${STORAGE_RC:-}" ]]; then
            overall="PASS"
            if [[ "${STORAGE_RC}" != "0" ]]; then
              overall="FAIL"
            fi
          fi

          rc_meaning="unknown"
          case "${STORAGE_RC:-}" in
            "")
              rc_meaning="missing"
              ;;
            0)
              rc_meaning="success"
              ;;
            97)
              rc_meaning="drill: intentional failure after storage check"
              ;;
            *)
              rc_meaning="remote failure (see logs)"
              ;;
          esac

          echo "## Remote Storage Health (Prod)" >> "$GITHUB_STEP_SUMMARY"
          echo "" >> "$GITHUB_STEP_SUMMARY"
          echo "- Overall: **${overall}**" >> "$GITHUB_STEP_SUMMARY"
          echo "- Remote exit code: \`${STORAGE_RC:-missing}\` (meaning: ${rc_meaning})" >> "$GITHUB_STEP_SUMMARY"
          echo "- Failure reason: \`${STORAGE_REASON:-missing}\`" >> "$GITHUB_STEP_SUMMARY"
          echo "- Drill fail: \`${DRILL_FAIL}\`" >> "$GITHUB_STEP_SUMMARY"
          echo "- Skip host sync: \`${SKIP_HOST_SYNC}\`" >> "$GITHUB_STEP_SUMMARY"
          echo "- Computed: df_used_pct=\`${DF_USED_PCT:-missing}\` (max=\`${MAX_FS_USED_PCT}\`), upload_gb=\`${UPLOAD_GB:-missing}\` (max=\`${MAX_UPLOAD_DIR_GB}\`), oldest_file_days=\`${OLDEST_FILE_DAYS:-missing}\` (max=\`${MAX_OLDEST_FILE_DAYS}\`), file_count=\`${FILE_COUNT:-missing}\`" >> "$GITHUB_STEP_SUMMARY"
          echo "" >> "$GITHUB_STEP_SUMMARY"

          echo "### Host Sync" >> "$GITHUB_STEP_SUMMARY"
          if [[ ! -f "$storage_log" ]]; then
            echo "" >> "$GITHUB_STEP_SUMMARY"
            echo "- Status: **UNKNOWN** (storage.log missing)" >> "$GITHUB_STEP_SUMMARY"
          else
            host_sync_block="$(
              awk '
                /^=== HOST SYNC START ===$/ { printing=1; next }
                /^=== HOST SYNC END ===$/ { printing=0 }
                printing { print }
              ' "$storage_log" | head -n 80
            )"

            if [[ -z "${host_sync_block}" ]]; then
              echo "" >> "$GITHUB_STEP_SUMMARY"
              echo "- Status: **UNKNOWN** (markers missing, see storage.log artifact)" >> "$GITHUB_STEP_SUMMARY"
            else
              host_sync_status="UNKNOWN"
              if echo "$host_sync_block" | grep -q "\\[host-sync\\] rc=0"; then
                host_sync_status="PASS"
              elif echo "$host_sync_block" | grep -q "\\[host-sync\\] rc="; then
                host_sync_status="FAIL"
              fi
              echo "" >> "$GITHUB_STEP_SUMMARY"
              echo "- Status: **${host_sync_status}**" >> "$GITHUB_STEP_SUMMARY"
              echo "" >> "$GITHUB_STEP_SUMMARY"
              echo '```' >> "$GITHUB_STEP_SUMMARY"
              echo "$host_sync_block" >> "$GITHUB_STEP_SUMMARY"
              echo '```' >> "$GITHUB_STEP_SUMMARY"
            fi
          fi

          echo "" >> "$GITHUB_STEP_SUMMARY"
          echo "### Storage Output" >> "$GITHUB_STEP_SUMMARY"
          if [[ ! -f "$storage_log" ]]; then
            echo "" >> "$GITHUB_STEP_SUMMARY"
            echo "- Status: **UNKNOWN** (storage.log missing)" >> "$GITHUB_STEP_SUMMARY"
          else
            status="UNKNOWN"
            if [[ -n "${STORAGE_RC:-}" ]]; then
              if [[ "${STORAGE_RC}" == "0" || "${STORAGE_RC}" == "97" ]]; then
                status="PASS"
              else
                status="FAIL"
              fi
            fi

            storage_block="$(
              awk '
                /^=== ATTENDANCE STORAGE START ===$/ { printing=1; next }
                /^=== ATTENDANCE STORAGE END ===$/ { printing=0; after=1; next }
                printing { print; next }
                after {
                  print
                  count++
                  if (count >= 20) exit
                }
              ' "$storage_log" | head -n 140
            )"

            if [[ -z "${storage_block}" ]]; then
              echo "" >> "$GITHUB_STEP_SUMMARY"
              echo "- Status: **UNKNOWN** (markers missing, see storage.log artifact)" >> "$GITHUB_STEP_SUMMARY"
              echo "" >> "$GITHUB_STEP_SUMMARY"
              echo "Last 80 log lines:" >> "$GITHUB_STEP_SUMMARY"
              echo '```' >> "$GITHUB_STEP_SUMMARY"
              tail -n 80 "$storage_log" >> "$GITHUB_STEP_SUMMARY" || true
              echo '```' >> "$GITHUB_STEP_SUMMARY"
            else
              echo "" >> "$GITHUB_STEP_SUMMARY"
              echo "- Status: **${status}**" >> "$GITHUB_STEP_SUMMARY"
              echo "" >> "$GITHUB_STEP_SUMMARY"
              echo '```' >> "$GITHUB_STEP_SUMMARY"
              echo "$storage_block" >> "$GITHUB_STEP_SUMMARY"
              echo '```' >> "$GITHUB_STEP_SUMMARY"
            fi
          fi

          if [[ "${overall}" == "FAIL" ]]; then
            echo "" >> "$GITHUB_STEP_SUMMARY"
            echo "### Suggested Remediation" >> "$GITHUB_STEP_SUMMARY"
            echo "" >> "$GITHUB_STEP_SUMMARY"
            case "${STORAGE_REASON:-}" in
              *FS_USAGE_TOO_HIGH*)
                echo "- Host filesystem usage is high; upload dir size is typically small in this case." >> "$GITHUB_STEP_SUMMARY"
                echo "- Run remote docker GC (reclaims unused images/caches; does not delete named volumes):" >> "$GITHUB_STEP_SUMMARY"
                echo '```bash' >> "$GITHUB_STEP_SUMMARY"
                echo "gh workflow run attendance-remote-docker-gc-prod.yml -f prune=true" >> "$GITHUB_STEP_SUMMARY"
                echo '```' >> "$GITHUB_STEP_SUMMARY"
                echo "- Then re-run this gate to confirm recovery." >> "$GITHUB_STEP_SUMMARY"
                ;;
              *UPLOAD_DIR_TOO_LARGE*|*OLDEST_FILE_TOO_OLD*)
                echo "- Upload directory drift detected; run cleanup dry-run first:" >> "$GITHUB_STEP_SUMMARY"
                echo '```bash' >> "$GITHUB_STEP_SUMMARY"
                echo "gh workflow run attendance-remote-upload-cleanup-prod.yml -f delete=false -f confirm_delete=false" >> "$GITHUB_STEP_SUMMARY"
                echo '```' >> "$GITHUB_STEP_SUMMARY"
                echo "- If safe, run destructive cleanup (requires explicit confirmation):" >> "$GITHUB_STEP_SUMMARY"
                echo '```bash' >> "$GITHUB_STEP_SUMMARY"
                echo "gh workflow run attendance-remote-upload-cleanup-prod.yml -f delete=true -f confirm_delete=true" >> "$GITHUB_STEP_SUMMARY"
                echo '```' >> "$GITHUB_STEP_SUMMARY"
                ;;
              HOST_SYNC_FAILED)
                echo "- Deploy-host git sync failed. Fix host sync or run with \`skip_host_sync=true\` for debugging." >> "$GITHUB_STEP_SUMMARY"
                ;;
              SSH_FAILED)
                echo "- SSH failed (rc=255). Check deploy secrets/host connectivity." >> "$GITHUB_STEP_SUMMARY"
                ;;
              *)
                echo "- See storage output and artifacts; rerun after fix." >> "$GITHUB_STEP_SUMMARY"
                ;;
            esac
          fi

          echo "" >> "$GITHUB_STEP_SUMMARY"
          echo "### Artifacts" >> "$GITHUB_STEP_SUMMARY"
          echo "" >> "$GITHUB_STEP_SUMMARY"
          echo "- Storage logs artifact: \`${artifact_name}\`" >> "$GITHUB_STEP_SUMMARY"
          echo "- Local evidence path (after download): \`output/playwright/ga/${GITHUB_RUN_ID}/storage.log\`" >> "$GITHUB_STEP_SUMMARY"
          echo "" >> "$GITHUB_STEP_SUMMARY"
          echo "Download:" >> "$GITHUB_STEP_SUMMARY"
          echo '```bash' >> "$GITHUB_STEP_SUMMARY"
          echo "gh run download ${GITHUB_RUN_ID} -n \"${artifact_name}\" -D \"output/playwright/ga/${GITHUB_RUN_ID}\"" >> "$GITHUB_STEP_SUMMARY"
          echo '```' >> "$GITHUB_STEP_SUMMARY"

          echo "" >> "$GITHUB_STEP_SUMMARY"
          echo "### Runbooks" >> "$GITHUB_STEP_SUMMARY"
          echo "" >> "$GITHUB_STEP_SUMMARY"
          echo "- Daily gates handbook: ${repo_url}/blob/main/docs/attendance-production-ga-daily-gates-20260209.md" >> "$GITHUB_STEP_SUMMARY"
          echo "- Production delivery checklist: ${repo_url}/blob/main/docs/attendance-production-delivery-20260207.md" >> "$GITHUB_STEP_SUMMARY"

          mkdir -p output/storage
          cp "$GITHUB_STEP_SUMMARY" output/storage/step-summary.md

      - name: Upload storage artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: attendance-remote-storage-prod-${{ github.run_id }}-${{ github.run_attempt }}
          retention-days: 14
          path: |
            output/storage/**

      - name: Fail when remote storage failed
        if: always()
        run: |
          rc="${{ steps.remote_storage.outputs.storage_rc }}"
          if [[ -z "$rc" ]]; then
            echo "storage_rc missing; failing workflow." >&2
            exit 1
          fi
          if [[ "$rc" != "0" ]]; then
            echo "Remote storage health check failed: rc=$rc" >&2
            exit 1
          fi

  issue:
    name: Storage Issue Alert (P1)
    needs: [storage]
    if: always()
    runs-on: ubuntu-latest
    timeout-minutes: 5
    env:
      DRILL_FAIL: ${{ github.event.inputs.drill_fail || 'false' }}
      ISSUE_TITLE_OVERRIDE: ${{ github.event.inputs.issue_title || '' }}
      SKIP_HOST_SYNC: ${{ github.event.inputs.skip_host_sync || 'false' }}
      STORAGE_RESULT: ${{ needs.storage.result }}
      STORAGE_RC: ${{ needs.storage.outputs.storage_rc }}
      STORAGE_REASON: ${{ needs.storage.outputs.storage_reason }}
      DF_USED_PCT: ${{ needs.storage.outputs.df_used_pct }}
      UPLOAD_GB: ${{ needs.storage.outputs.upload_gb }}
      FILE_COUNT: ${{ needs.storage.outputs.file_count }}
      OLDEST_FILE_DAYS: ${{ needs.storage.outputs.oldest_file_days }}
    steps:
      - name: Open/update issue on failure (P1, no paging)
        uses: actions/github-script@v7
        with:
          script: |
            const drillFail = String(process.env.DRILL_FAIL || '').trim() === 'true'
            const skipHostSync = String(process.env.SKIP_HOST_SYNC || '').trim()
            const storageResult = String(process.env.STORAGE_RESULT || '').trim()
            const storageRc = String(process.env.STORAGE_RC || '').trim()
            const storageReason = String(process.env.STORAGE_REASON || '').trim()
            const dfUsedPct = String(process.env.DF_USED_PCT || '').trim()
            const uploadGb = String(process.env.UPLOAD_GB || '').trim()
            const fileCount = String(process.env.FILE_COUNT || '').trim()
            const oldestFileDays = String(process.env.OLDEST_FILE_DAYS || '').trim()
            const titleOverride = String(process.env.ISSUE_TITLE_OVERRIDE || '').trim()

            const defaultTitle = '[Attendance P1] Storage health alert'
            const title = titleOverride || defaultTitle

            const debugRun = skipHostSync === 'true'

            // Avoid creating drill/debug issues unless explicitly requested via override title.
            const allowIssueOps = (!drillFail && !debugRun) || Boolean(titleOverride)
            if (!allowIssueOps) {
              core.info('Drill/debug run: skipping issue operations (no override title).')
              return
            }

            const owner = context.repo.owner
            const repo = context.repo.repo
            const runUrl = `${context.serverUrl}/${owner}/${repo}/actions/runs/${context.runId}`
            const artifactName = `attendance-remote-storage-prod-${context.runId}-${process.env.GITHUB_RUN_ATTEMPT || '1'}`

            const failed = storageResult === 'failure'
            const statusLine = `job: storage=${storageResult || 'unknown'} rc=${storageRc || 'missing'} reason=${storageReason || 'missing'} df_used_pct=${dfUsedPct || 'missing'} upload_gb=${uploadGb || 'missing'} oldest_file_days=${oldestFileDays || 'missing'} file_count=${fileCount || 'missing'} skip_host_sync=${skipHostSync || 'missing'}${drillFail ? ' (drill_fail=true)' : ''}`

            const body = [
              'Automated storage health tracking issue (P1, no paging).',
              '',
              `Run: ${runUrl}`,
              statusLine,
              '',
              'Suggested remediation:',
              '- If `reason` includes `FS_USAGE_TOO_HIGH`: run `Attendance Remote Docker GC (Prod)` and rerun `Storage Health`.',
              '- If `reason` includes `UPLOAD_DIR_TOO_LARGE` or `OLDEST_FILE_TOO_OLD`: run `Remote Upload Cleanup (Prod)` (dry-run first), then rerun `Storage Health`.',
              '',
              `Artifacts: ${artifactName}`,
              '',
              'Download:',
              '```bash',
              `gh run download ${context.runId} -n \"${artifactName}\" -D \"output/playwright/ga/${context.runId}\"`,
              '```',
            ].join('\\n')

            const q = `repo:${owner}/${repo} type:issue in:title \"${title}\"`
            const search = await github.rest.search.issuesAndPullRequests({
              q,
              sort: 'updated',
              order: 'desc',
              per_page: 20,
            })
            const items = Array.isArray(search.data.items) ? search.data.items : []
            const match = items.find((item) => item && !item.pull_request && item.title === title) || null

            if (failed) {
              if (match) {
                if (match.state !== 'open') {
                  await github.rest.issues.update({
                    owner,
                    repo,
                    issue_number: match.number,
                    state: 'open',
                    body,
                  })
                  core.info(`Reopened issue #${match.number}`)
                } else {
                  await github.rest.issues.createComment({
                    owner,
                    repo,
                    issue_number: match.number,
                    body,
                  })
                  core.info(`Commented on issue #${match.number}`)
                }
                return
              }

              const created = await github.rest.issues.create({
                owner,
                repo,
                title,
                body,
              })
              core.info(`Created issue #${created.data.number}`)
              return
            }

            // Recovery: close an existing open issue when the storage gate passes.
            if (!match) {
              core.info('No storage health issue found; nothing to close.')
              return
            }
            if (match.state !== 'open') {
              core.info(`Storage health issue exists but is already closed (#${match.number}).`)
              return
            }

            await github.rest.issues.createComment({
              owner,
              repo,
              issue_number: match.number,
              body: [
                'Storage health recovery detected. Closing issue.',
                '',
                `Run: ${runUrl}`,
                statusLine,
              ].join('\\n'),
            })
            await github.rest.issues.update({
              owner,
              repo,
              issue_number: match.number,
              state: 'closed',
            })
            core.info(`Closed issue #${match.number}`)
